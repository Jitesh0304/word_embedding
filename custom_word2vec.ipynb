{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBoW model (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChiefComplaint</th>\n",
       "      <th>Symptoms</th>\n",
       "      <th>Medicines</th>\n",
       "      <th>InvestigationName</th>\n",
       "      <th>Paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ho intermittent pain abdomen and flatulence X ...</td>\n",
       "      <td>itching , vomiting , fatigue , weight_loss , h...</td>\n",
       "      <td>NEOROF 1 20ML INJECTION, URSODIL 300MG TABLETS</td>\n",
       "      <td>PACEMAKER ( DUAL ), MRI THORACIC SPINE (FULL S...</td>\n",
       "      <td>Ho intermittent pain abdomen and flatulence X ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>During routine health checkup gallstone detect...</td>\n",
       "      <td>fatigue , weight_loss , restlessness , sweatin...</td>\n",
       "      <td>RYZODEG FLEXTOUCH PEN, COVATIL 250MG TABS CEFU...</td>\n",
       "      <td>( BED SIDE ) COLOR DOPPLER (CAROTID), GGT, MRI...</td>\n",
       "      <td>During routine health checkup gallstone detect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ChiefComplaint  \\\n",
       "0  Ho intermittent pain abdomen and flatulence X ...   \n",
       "1  During routine health checkup gallstone detect...   \n",
       "\n",
       "                                            Symptoms  \\\n",
       "0  itching , vomiting , fatigue , weight_loss , h...   \n",
       "1  fatigue , weight_loss , restlessness , sweatin...   \n",
       "\n",
       "                                           Medicines  \\\n",
       "0     NEOROF 1 20ML INJECTION, URSODIL 300MG TABLETS   \n",
       "1  RYZODEG FLEXTOUCH PEN, COVATIL 250MG TABS CEFU...   \n",
       "\n",
       "                                   InvestigationName  \\\n",
       "0  PACEMAKER ( DUAL ), MRI THORACIC SPINE (FULL S...   \n",
       "1  ( BED SIDE ) COLOR DOPPLER (CAROTID), GGT, MRI...   \n",
       "\n",
       "                                           Paragraph  \n",
       "0  Ho intermittent pain abdomen and flatulence X ...  \n",
       "1  During routine health checkup gallstone detect...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "file_path = r\"C:\\Users\\jites\\Desktop\\Project_folder\\complaint_data_4_columns.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df = df.astype(str)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent.split() for sent in df['Paragraph'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Build the vocabulary and encode the words as integers\n",
    "vocab = Counter(word for sentence in sentences for word in sentence)\n",
    "word2idx = {word: i for i, (word, _) in enumerate(vocab.items())}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5901\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sentences, word2idx, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in sentences:\n",
    "        indices = [word2idx[word] for word in sentence]\n",
    "        for center_word_pos in range(len(indices)):\n",
    "            for offset in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + offset\n",
    "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                pairs.append((indices[center_word_pos], indices[context_word_pos]))\n",
    "    return pairs\n",
    "\n",
    "training_data_pair = generate_training_data(sentences, word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321736"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CBOWModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_size):\n",
    "#         super(CBOWModel, self).__init__()\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "#         self.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "#     def forward(self, context):\n",
    "#         context_embeds = self.embeddings(context).sum(dim=1)\n",
    "#         output = self.linear(context_embeds)\n",
    "#         return output\n",
    "\n",
    "\n",
    "class Word2VecModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2VecModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, center_word):\n",
    "        embeds = self.embeddings(center_word)\n",
    "        output = self.linear(embeds)\n",
    "        return output\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 100\n",
    "\n",
    "# Instantiate the model\n",
    "model = Word2VecModel(vocab_size, embedding_dim).to(device= device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to tensors\n",
    "training_data_tensor_list = [(torch.tensor(center, dtype=torch.long, device=device), torch.tensor(context, dtype=torch.long, device=device))\n",
    "                  for center, context in training_data_pair]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321736"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1461307.245071739\n",
      "Epoch 2, Loss: 1283014.0503216684\n",
      "Epoch 3, Loss: 1219265.5340221524\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i,(center_word, context_word) in enumerate(training_data_tensor_list):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(center_word.unsqueeze(0))\n",
    "        loss = criterion(output, context_word.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector for '(': tensor([ 0.4380,  0.0652,  0.6872, -1.6896,  1.1090, -1.5445,  0.5307,  0.5110,\n",
      "        -1.0391,  0.3894,  1.6048, -1.6513,  0.3369,  0.7499, -0.2149,  1.2155,\n",
      "         1.0319, -1.7604,  0.4340,  0.8499, -0.9372, -1.1845,  2.6167,  0.3947,\n",
      "         0.2730,  0.9358,  0.3197,  0.2985,  0.4007,  0.1425, -2.6224,  0.7531,\n",
      "         0.6541, -0.0580, -0.3898, -0.9394, -1.1541, -0.6402, -1.3745,  1.4276,\n",
      "        -1.3954, -1.3844,  1.1818,  0.0890, -0.2538, -0.1800, -0.1370,  1.0432,\n",
      "         0.0836, -0.7071, -0.1433,  0.1105, -0.0124, -0.5025,  0.5373, -0.0951,\n",
      "        -0.6760,  0.4060, -0.5962, -0.5114, -0.7854,  0.2070,  0.8996, -1.6358,\n",
      "        -3.6977, -0.0247,  0.0085, -0.4834,  0.2817, -1.6896, -0.1866, -0.3705,\n",
      "        -1.1271, -0.3929, -0.6774, -0.4786, -0.8275, -0.2733,  1.5070, -0.3191,\n",
      "         1.1795,  0.6773, -0.4716, -1.1100, -0.4058, -0.2168,  1.0003, -0.2376,\n",
      "        -0.8260, -0.1713,  1.5718,  0.1994, -0.7791, -1.4182, -0.3075, -1.1429,\n",
      "         0.7394,  0.4833,  1.3745,  0.2092], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "word = \"(\"\n",
    "word_idx = word2idx[word]\n",
    "embedding_vector = model.embeddings(torch.tensor(word_idx, device=device)).detach()\n",
    "print(f\"Embedding vector for '{word}': {embedding_vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# model_path = \"custom_word2vec_model.pth\"\n",
    "# vocab_path = \"vocab.pkl\"\n",
    "\n",
    "#     ### Save the model state dictionary\n",
    "# torch.save(model.state_dict(), model_path)\n",
    "\n",
    "#     ### Save the vocabulary mappings\n",
    "# with open(vocab_path, 'wb') as f:\n",
    "#     pickle.dump((word2idx, idx2word), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"custom_word2vec_model.pth\"\n",
    "# vocab_path = \"vocab.pkl\"\n",
    "\n",
    "#     ### Load the vocabulary mappings\n",
    "# with open(vocab_path, 'rb') as f:\n",
    "#     word2idx, idx2word = pickle.load(f)\n",
    "\n",
    "#     ### Recreate the model instance\n",
    "# vocab_size = len(word2idx)\n",
    "# embedding_dim = 100            ## This should match the dimension used during training\n",
    "# model = Word2VecModel(vocab_size, embedding_dim)\n",
    "\n",
    "#     ### Load the saved state dictionary into the model\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "#     ### Set the model to evaluation mode (optional, depends on your use case)\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-grams model (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_training_data(sentences, word2idx, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in sentences:\n",
    "        indices = [word2idx[word] for word in sentence]\n",
    "        for center_word_pos in range(len(indices)):\n",
    "            center_word = indices[center_word_pos]\n",
    "            context_words = []\n",
    "            for offset in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + offset\n",
    "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                context_words.append(indices[context_word_pos])\n",
    "            for context_word in context_words:\n",
    "                pairs.append((center_word, context_word))\n",
    "    return pairs\n",
    "\n",
    "training_data_skip_gram_pair = generate_skipgram_training_data(sentences, word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, center_word):\n",
    "        embeds = self.embeddings(center_word)\n",
    "        output = self.linear(embeds)\n",
    "        return output\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model = SkipGramModel(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to tensors\n",
    "training_data_skip_gram_tensor_list = [(torch.tensor(center, dtype=torch.long, device=device), \n",
    "                                        torch.tensor(context, dtype=torch.long, device=device)) \n",
    "                                       for center, context in training_data_skip_gram_pair]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1460225.8742566854\n",
      "Epoch 2, Loss: 1282904.5380657166\n",
      "Epoch 3, Loss: 1219169.7831053138\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center_word, context_word in training_data_skip_gram_tensor_list:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(center_word.unsqueeze(0))\n",
    "        loss = criterion(output, context_word.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector for 'fatigue': tensor([ 0.6784, -1.6613,  0.9200,  1.1474, -0.1259,  0.0896, -1.5391, -0.2119,\n",
      "         0.0292,  1.0094,  0.2427,  0.8272,  0.8194,  1.3292, -0.0508,  1.0731,\n",
      "        -0.3961,  0.0055,  0.8425,  0.2499, -1.3036,  0.1286,  0.8677, -1.2540,\n",
      "         0.7143,  1.3858,  0.5125, -0.5405, -0.5231, -0.5525,  1.0788, -0.4218,\n",
      "        -0.7048, -1.0020, -0.6864,  1.0180,  1.0795,  1.2585,  0.2061, -0.7169,\n",
      "         0.5437,  1.3036,  1.0163, -0.3995, -1.1355,  0.6984,  0.6052, -0.4189,\n",
      "         0.2060,  1.1750,  1.8251, -0.4229,  0.1029, -0.0470,  0.4065, -0.2243,\n",
      "        -1.1510,  0.2954, -0.7789,  0.7182, -0.2550, -0.9052,  0.4521, -0.7722,\n",
      "         0.7700, -0.7393, -0.3932,  0.3719,  0.7401,  2.9336,  0.9360,  0.0822,\n",
      "         0.7936,  0.4257,  0.4557, -0.2305, -0.1107,  0.4488,  0.0765,  0.4439,\n",
      "        -0.0958,  0.6893, -1.1718, -0.2316,  0.2174,  1.4285,  0.8127,  0.8827,\n",
      "        -0.5775,  2.3087,  0.2505,  1.1557,  1.7782,  0.7352, -1.4089, -0.0446,\n",
      "        -0.2680,  0.5760,  1.8844,  1.0769])\n"
     ]
    }
   ],
   "source": [
    "word = \"fatigue\"\n",
    "word_idx = word2idx[word]\n",
    "embedding_vector = model.embeddings(torch.tensor(word_idx, device=device)).detach()\n",
    "print(f\"Embedding vector for '{word}': {embedding_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "While the underlying architecture of the CBOW and Skip-gram models might look similar when implemented, they fundamentally differ in \n",
    "how they approach learning word representations and in the details of their training objectives. Let's explore the differences.\n",
    "\n",
    "1. Objective Function:\n",
    "    CBOW (Continuous Bag of Words):\n",
    "        Objective: Predict the center word given the context words.\n",
    "        Input: Multiple context words (within a window size) surrounding a target word.\n",
    "        Output: The model predicts the probability distribution over the vocabulary, aiming to maximize the probability of the \n",
    "                correct center word given the context words.\n",
    "        Loss Function: The loss is calculated between the predicted center word and the actual center word.\n",
    "    The CBOW model tends to be faster to train because it averages the embeddings of multiple context words and predicts a \n",
    "    single word (center word).\n",
    "\n",
    "    Skip-gram:\n",
    "        Objective: Predict context words given a single center word.\n",
    "        Input: A single center word.\n",
    "        Output: The model predicts multiple context words within the window size.\n",
    "        Loss Function: The loss is calculated between the predicted context words and the actual context words. The model is trained \n",
    "                        to maximize the probability of predicting the correct context words for a given center word.\n",
    "    The Skip-gram model typically requires more computations because it predicts multiple words (context words) for each input (center word).\n",
    "\n",
    "2. Data Preparation:\n",
    "    CBOW:\n",
    "        The model is trained with input consisting of multiple context words and the target is the center word.\n",
    "        Example: For the sentence \"The cat sits on the mat,\" with a window size of 2, the model may see [the, sits] as input \n",
    "                and cat as the output.\n",
    "    Skip-gram:\n",
    "        The model is trained with input consisting of a single center word and the target is the surrounding context words.\n",
    "        Example: For the sentence \"The cat sits on the mat,\" with a window size of 2, the model may see cat as input and [the, sits] \n",
    "                as outputs.\n",
    "\n",
    "3. Training Process:\n",
    "    CBOW:\n",
    "        The model aggregates the embeddings of context words (often by averaging) and uses them to predict the center word.\n",
    "        Example:\n",
    "            Input: [the, sits]\n",
    "            Model: Embedding + Average (or sum) + Linear + Softmax\n",
    "            Output: Probability distribution over the vocabulary for predicting the word \"cat.\"\n",
    "    Skip-gram:\n",
    "        The model takes a single word and predicts the surrounding context words one by one.\n",
    "        Example:\n",
    "            Input: cat\n",
    "            Model: Embedding + Linear + Softmax\n",
    "            Output: Probability distribution over the vocabulary for predicting each word in the context window [the, sits].\n",
    "\n",
    "4. Efficiency and Suitability:\n",
    "    CBOW:\n",
    "        Typically faster to train.\n",
    "        More suitable when your dataset is smaller or when speed is a priority.\n",
    "        Tends to smooth the representations of words because of the averaging step.\n",
    "    Skip-gram:\n",
    "        Typically slower to train because it predicts multiple outputs per input word.\n",
    "        More suitable for large datasets and when detailed, high-quality word representations are desired.\n",
    "        Often better at capturing rare word representations since each word is treated independently.\n",
    "\n",
    "5. Architecture Overview:\n",
    "    Despite the similar architecture (an embedding layer followed by a linear layer), the primary difference lies in:\n",
    "\n",
    "    Input/Output handling:\n",
    "        CBOW uses multiple words as input to predict a single word.\n",
    "        Skip-gram uses a single word as input to predict multiple words.\n",
    "    Training Focus:\n",
    "        CBOW is context-to-center focused.\n",
    "        Skip-gram is center-to-context focused.\n",
    "\n",
    "    Visual Comparison:\n",
    "        CBOW:\n",
    "            Input: [context1, context2, ... contextN]\n",
    "            Model: Embedding -> Aggregate (average/sum) -> Linear -> Softmax\n",
    "            Output: center word\n",
    "        Skip-gram:\n",
    "            Input: center word\n",
    "            Model: Embedding -> Linear -> Softmax\n",
    "            Output: [context1, context2, ... contextN]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
